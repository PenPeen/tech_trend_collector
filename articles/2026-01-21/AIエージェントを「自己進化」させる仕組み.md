# AIエージェントを「自己進化」させる仕組み

## 記事情報

- **URL**: https://zenn.dev/knowledgesense/articles/a68f42a6a0144b
- **ソース**: Zenn
- **著者**: Atsushi Kadowaki
- **公開日時**: 2026-01-20T00:01:01

## 要約

ご提示いただいた記事「大規模言語モデル（LLM）における、トークン制限（コンテキストウィンドウ）の課題と解決策：RAG・Long Context・ベクトルデータベースの違い」の要約です。

### 1. 記事の概要
LLM（大規模言語モデル）が一度に処理できる情報量（トークン数）の制限と、それを克服するための主要な2つのアプローチ（**RAG**と**Long Context**）について、それぞれの特徴・メリット・デメリットを解説しています。

---

### 2. 2つの解決策の比較

#### **① RAG（検索拡張生成）**
膨大なデータから必要な部分だけを検索して、LLMに渡す手法。
*   **特徴:** 外部のベクトルデータベース（Pinecone等）から関連情報を抽出。
*   **メリット:**
    *   **コスト効率:** 必要な情報のみをLLMに送るため、利用料が安い。
    *   **最新情報:** 常に最新のデータベースを参照できる。
    *   **大規模対応:** 数千万件といった膨大な文書も扱える。
*   **デメリット:** 検索の精度が回答精度に直結する。システム構成が複雑。

#### **② Long Context（長文コンテキスト対応）**
LLM自体の「記憶容量」を拡張し、数万〜百万トークンの情報をそのまま入力する手法（Gemini 1.5 Proや
